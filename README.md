# Overfitting & Underfitting

When we talk about the **Machine Learning** model, we actually talk about how well it performs and its accuracy which is known as prediction errors.  Let us consider that we are designing a machine learning model. A model is said to be a good machine learning model if it generalizes any new input data from the problem domain in a proper way.
This helps us to make predictions about future data, that the data model has never seen.

Now, suppose we want to check how well our machine learning model learns and generalizes to the new data. For that, we have overfitting and underfitting, which are majorly responsible for the poor performances of the machine learning algorithms.

### Reasons for Overfitting
- Data used for training is not cleaned and contains noise (garbage values) in it
- The model has a high variance
- The size of the training dataset used is not enough
- The model is too complex

### Reasons for Underfitting
- Data used for training is not cleaned and contains noise (garbage values) in it
- The model has a high bias
- The size of the training dataset used is not enough
- The model is too simple

#

![image](https://user-images.githubusercontent.com/92504503/198991401-060e6550-0619-4af9-8866-d3549f8ecc5a.png)

![image](https://miro.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png)

#

Note:

**Noise** > Noise is Unnecessary and irrelevant data that reduces the performance of the model.


![image](https://user-images.githubusercontent.com/92504503/198994417-0cca5511-e4cb-4d65-96ab-487eac8888b5.png)
